# Slot_Machine_DQN
ìŠ¬ë¡¯ë¨¸ì‹  ë¬¸ì œ - DQN êµ¬í˜„ 

## intro
ì•„ë˜ ë¬¸ì„œì—ì„œëŠ” ê°•í™”ì¸ë“¤ì˜ ì½”ë“œ ìŠ¤ë‹ˆí«ë“¤ì´ ëª¨ì—¬ ìˆìŠµë‹ˆë‹¤.  
ğŸ’¡ : **ê°œì„ ì ì´ ì¡´ì¬**  
ğŸ‘ : **ë©‹ì§„ êµ¬í˜„ !**  
ì˜ ê¸°í˜¸ë¥¼ í†µí•´ ì½”ë“œ ë¦¬ë·°ê°€ ì§„í–‰ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  

*ì£¼ì˜ì‚¬í•­* : ì½”ë“œ ë°œì·Œê¸°ì¤€ì€ ì˜í•˜ê³  ë§ê³ ê°€ ì•„ë‹™ë‹ˆë‹¤! ê·¸ëƒ¥ ëˆˆì— ë„ëŠ”ëŒ€ë¡œ ê°€ì ¸ì˜¨ê±°ë‹ˆ, ì½”ë“œ ë¦¬ë·°ê°€ ë§ë‹¤ê³  ì£¼ëˆ…ë“œì‹¤ í•„ìš”ê°€ ì €ì–¸í˜€ ì—†ìŠµë‹ˆë‹¤! 

## Net
- Keras ê³„ì—´ì˜ ì½”ë“œ ì‘ì„± ë°©ë²•  

```python
@ ì§€ì€ë²—
class DQN(nn.Module) :
    def __init__(self):

        super().__init__()
        self.Linear = nn.Sequential(nn.Linear(3,5), # ìƒíƒœ ì…ë ¥
                                    nn.ReLU(),
                                    nn.Linear(5,5),
                                    nn.ReLU(),
                                    nn.Linear(5,5),
                                    nn.ReLU(),
                                    nn.Linear(5,3)) # ì™¼, ê°€, ì˜¤
    def forward(self, x):
        x = self.Linear(x)
        return x
```

ğŸ’¡ **namingì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ì!**  
self.Linearì€ nn.Linearì´ë¼ëŠ” ë§¤ì†Œë“œê°€ ì¡´ì¬í•˜ë‹¤ë³´ë‹ˆ ì¡°ê¸ˆ í—·ê°ˆë¦´ ìˆ˜ ìˆë‹¤. 

- pytorch ê³„ì—´ì˜ ì½”ë“œ ì‘ì„± ë°©ë²•  

```python
@ ì£¼í˜„ë²—
class DQN(nn.Module):
  def __init__(self, state_size, action_size):
    super().__init__()
    self.action_size = action_size
    self.state_size = state_size

    self.fc1 = nn.Linear(state_size, 30)
    self.fc2 = nn.Linear(30, 30)
    self.fc3 = nn.Linear(30, action_size)

  def forward(self, x):
    x = self.fc1(x)
    x = self.fc2(x)
    x = self.fc3(x)
    return x
```

ğŸ‘ **ì¼ë°˜í™”ì‹œí‚¨ ì½”ë“œ!** : `state_size`ì™€ `action_size`ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë‹¤ë£¨ê¸° ë•Œë¬¸ì—, í´ë˜ìŠ¤ ë‚´ë¶€ì˜ ì½”ë“œë¥¼ ê±´ë“¤ì§€ ì•Šë”ë¼ë„ ì‰½ê²Œ ìˆ˜ì •ì´ ê°€ëŠ¥í•˜ë‹¤.  

## Agent 
### env-agent ì˜ì¡´ì„±
```python
@ ì€ë‚˜ë²— ë„í¬ë²— ì§¬ë½• 
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size

# ì¤‘ëµ

lever = Lever()
agent = DQNAgent(state_size=3, action_size=3)
```

```python
@ ì§€ë¯¼ë²—
class MonkeyAgent:
    def __init__(self, env):
        self.env = env
        self.n_actions = self.env.n_actions
```
ğŸ’¡
envì™€ agentëŠ” ë§ë¬¼ë ¤ ìˆë‹¤. envì˜ stateì™€ action_spaceì˜ í¬ê¸°ê°€ agentì˜ ì‹ ê²½ë§ì˜ input, output nodeì˜ ìˆ˜ë¥¼ ê²°ì •í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë¬¼ë¡  ì™¸ë¶€ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì•„ë„ ë˜ì§€ë§Œ ê·¸ê²ƒë³´ë‹¤ëŠ” envë¥¼ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ë°›ê³ , í•„ìš”í•œ ì–´íŠ¸ë¦¬ë·°íŠ¸ë“¤ì„ Agent class ë‚´ë¶€ì—ì„œ ë°›ëŠ” í˜•ì‹ì´ ë” ë‚«ë‹¤. 

```python
@ë„í¬ë²— ì½”ë“œ ìˆ˜ì •
lever = Lever()
agent = DQNAgent(state_size=len(lever.state), action_size=lever.num_actions)
```

```python
@ì§€ë¯¼ë²—
env = LeverEnv()
agent = MonkeyAgent(env)
```

### HYPER_PARAMETERS
```python
@ ìŠ¹ì—°ë²—
self.discount_factor = 0.99
self.learning_rate = 0.001

self.epsilon = 1
self.epsilon_decay = 0.999
self.epsilon_min = 0.01

self.batchsize = 64 # self.batch_size = 64
```

```python
@ ì§€ë¯¼ë²—
MEM_SIZE = 1000
MEM_SIZE_MIN = 100

BATCH_SIZE = 16
LEARNING_RATE = 0.001
DISCOUNT = 0.1

EPSILON = 0.9
EPSILON_MIN = 0 # 0.01
EPSILON_DECAY = 0.999

UPDATE_TARGET_EVERY = 5
```

```python
@ ì •ì—°ë²—
def train_model(self):
        global EPSILON
        if EPSILON > EPSILON_MIN:
            EPSILON *= EPSILON_DECAY
```
ğŸ’¡  
í•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” ëŒ€ë¬¸ìë¡œ í‘œí˜„í•˜ë©°, class ì™¸ë¶€ì—ì„œ ë¯¸ë¦¬ ì§€ì •í•´ë‘ëŠ”ê²Œ ì¼ë°˜íšŒëœ í˜•íƒœì…ë‹ˆë‹¤! ì „ì—­ ë³€ìˆ˜ì´ê¸°ì— class ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, globalì„ ì´ìš©í•´ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ”ê±´ ì§€ì–‘í•˜ëŠ”ê²Œ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—¡ì‹¤ë¡ ì˜ ê²½ìš°, ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ê°’ì´ê¸° ë•Œë¬¸ì— ë³€ìˆ˜ë¥¼ ë”°ë¡œ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì—­ ë³€ìˆ˜ì˜ ê°’ì´ ë°”ë€Œê³ , ì´ëŠ” ë‹¤ìŒ ì‹¤í–‰ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ë¥¼ ë°œìƒí•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `__init__` ë‹¨ì—ì„œ `self.epsilon`ê°™ì€ ì–´íŠ¸ë¦¬ë·°íŠ¸ë¡œ ì •ì˜í•´ì£¼ì„¸ìš”!  
ë˜í•œ ì´ í…ŒìŠ¤í¬ì˜ ê²½ìš° ë¬¸ì œê°€ ë‹¨ìˆœí•˜ì—¬ ë§ì€ íƒí—˜ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•´ ì €ëŠ” EPSILON_MIN = 0ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ìœ„ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë“¤ì€ (+lr ë¶€ë¶„) ê°•í™”í•™ìŠµì— ìˆì–´ í•µì‹¬ì ì¸ ë¶€ë¶„ë“¤ì´ë‹ˆ, í…ŒìŠ¤í¬ì— ë§ëŠ” ì„¤ì •ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. 

### get_action
```python
@ ì§€ì€ë²—
    def get_action(self, state, idx,train=True):

        if train==False :
            with torch.no_grad():
                 q_value = self.model(torch.tensor(state, dtype=torch.float32 ))
            return torch.argmax(q_value).item()

        if train==True and np.random.rand() <= EPSILON:
            # SEARCH_IDX.append(idx)
            return (np.random.randint(0,3))

        else :
            q_value = self.model(torch.tensor(state, dtype=torch.float32 ))
            return torch.argmax(q_value).item()
```

ğŸ‘ **train, testì— ëŒ€í•´ ì •í™•íˆ ì•Œê³  ìˆì–´ìš”!** : trainì—ì„œëŠ” íƒí—˜ìœ¨ì„ ì‚´ë ¤ì£¼ë©° ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œ ì¸í•œ ì˜ëª»ëœ ìˆ˜ë ´ì„ ë§‰ëŠ”ë‹¤. ë°˜ëŒ€ë¡œ testëŠ” í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— íƒí—˜ìœ¨ì´ ì¡´ì¬í•˜ë©´ ì•ˆëœë‹¤. ë”°ë¼ì„œ trainê³¼ testë¥¼ êµ¬ë¶„í•´ testì—ì„œëŠ” randomí•œ íƒí—˜ì„ ì—†ì•´ë‹¤.  

=> ì´ë ‡ê²Œ êµ¬í˜„í•˜ì§€ ì•Šìœ¼ë©´ trainì€ ì–´ë–»ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆì„ê¹Œ?  
A :  

ğŸ’¡ **idxëŠ” í–‰ë™ì„ ê²°ì •í•˜ëŠ”ë° í•„ìš”í•œ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë‹˜!**

```python
@ ì •ì—°ë²—
    def get_action(self, state):
        if np.random.rand() <= EPSILON:
            action = np.random.choice(range(self.action_size))
        else:
            q_value = self.model(state)
            action = torch.argmax(q_value).item()

        return action
```

```python
@ ì£¼í˜„ë²—
    def get_action(self, state):
        if np.random.rand() <= self.epsilon:
          return np.random.choice(self.action_size)

      state = torch.tensor(state, dtype=torch.float32).to(self.device)
      q_values = self.model(state)
      return torch.argmax(q_values).item()
```

=> `np.random.choice` ëŠ” íŒŒë¼ë¯¸í„°ë¡œ 1_dim, int ë‘˜ ë‹¤ ë°›ì„ ìˆ˜ ìˆë‹¤.  
ì°¸ê³  : https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html

### Replay_Memory
```python
@ ì§€ì€ë²—
    self.memory=deque(maxlen=MEM_SIZE)

    # ì¤‘ê°„ ìƒëµ

    def append_sample(self, state,action,reward, next_state,done):
        if(len(self.memory)>MEM_SIZE):
            self.memory.pop()
        self.memory.append((state,action,reward, next_state, done))
```

ğŸ’¡ : queue êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— popì„ í•´ì¤„ í•„ìš”ê°€ ì—†ë‹¤!   

```python
@ ìŠ¹ì—°ë²—
def append_sample(self, state, action, reward, next_state, done):
        '''
        ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì— ìƒ˜í”Œ ì €ì¥
        '''
        self.memory.append((state,action,reward,next_state,done))
```

ğŸ‘ : ì •ì„ì ì¸ ì½”ë“œ ğŸŒ¸   

### Sample -> Trainable Data
```python
@ ìŠ¹ì—°ë²— ì •ì—°ë²— ë¯¼ì„œë²—
# ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ìƒ˜í”Œ ë¬´ì‘ìœ„ ì¶”ì¶œ
minibatch = random.sample(self.memory, self.batchsize)

states = torch.tensor([sample[0][i] for sample in minibatch for i in range(3)]).reshape(self.batchsize,3)
actions = torch.tensor([sample[1] for sample in minibatch])
rewards = torch.tensor([sample[2] for sample in minibatch])
next_states = torch.tensor([sample[3][i] for sample in minibatch for i in range(3)]).reshape(self.batchsize,3)
dones = torch.tensor([sample[4] for sample in minibatch]).float()
```

list compreshensionì„ ì‚¬ìš©í•˜ëŠ”ê±´ ì¼ë°˜ì ì¸ forë¬¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ì†ë„ê°€ ë¹ ë¥´ë‹¤! í•˜ì§€ë§Œ ë” ë¹ ë¥¸ ë°©ë²•ì´ ì¡´ì¬í•œë‹¤. 

```python
@ ì£¼í˜„ë²—
batch = random.sample(self.memory, BATCH_SIZE)
states, actions, rewards, next_states, dones = zip(*batch)  # ë©”ëª¨ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜´

states = torch.tensor(states, dtype=torch.float32).to(self.device)
actions = torch.tensor(actions, dtype=torch.long).to(self.device)
rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)
dones = torch.tensor(dones, dtype=torch.float32).to(self.device)
```

ğŸ‘ : `zip` í•¨ìˆ˜ëŠ” ì†ë„ê°œì„ ì— ë§¤ìš° ë§¤ìš° ì¢‹ì€ í•¨ìˆ˜! ğŸŒ¸   


### Train
- scalar type 
```python
@ ì§€ì€ë²—
# í˜„ì¬ ìƒíƒœì— ëŒ€í•œ ëª¨ë¸ì˜ íí•¨ìˆ˜
predicts = self.model(states)
one_hot_action= F.one_hot(actions, num_classes=3)
predicts = torch.sum(one_hot_action*predicts, axis=1)

# ë‹¤ìŒ ìƒíƒœì— ëŒ€í•œ íƒ€í‚· ëª¨ë¸ì˜ íí•¨ìˆ˜
target_predicts=self.target_model(next_states)

# ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì„ ì´ìš©í•œ íƒ€ê¹ƒ ì—…ë°ì´íŠ¸
max_q, _=torch.max(target_predicts, dim=-1)
targets = rewards + (~dones) * GAMMA * max_q
```

ğŸ‘ : `~dones` ê°„ì§€ë‚˜ìš”!, ë ˆí¼ëŸ°ìŠ¤ ì½”ë“œë¥¼ ì •í™•íˆ ì˜ ê°–ê³ ì˜¨ ì½”ë“œ!!  

ğŸ’¡ : `self.target_model(next_states)`ê°€ í•™ìŠµëŒ€ìƒì´ë©´ ì•ˆëœë‹¤!! ì§€ê¸ˆ ì½”ë“œëŠ” `self.target_model.eval()`ì´ë‚˜ `with torch.no_grad():`ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— target ê°’ë„ grad ë³€í™˜ì˜ ëŒ€ìƒì´ë‹¤. ì´ ë‘ ì½”ë“œë¥¼ ì¶”ê°€í•´ target_modelì— ëŒ€í•œ ì—­ì „íŒŒë¥¼ ë§‰ì•„ì•¼ í•œë‹¤.  

ğŸ‘‡ğŸ‘‡ **ì—­ì „íŒŒ ëŒ€ìƒì¸ì§€ í™•ì¸í•˜ëŠ” ë°©ë²•**  
```python
@ ì§€ë¯¼ë²—
print(target.requires_grad) # False
print(pred.requires_grad) # True
```
-> if ë‘ ê°œ ë‹¤ Falseë¼ë©´, ì˜¤ë¥˜ ë°œìƒ  

```python
@ ì§€ë¯¼ë²— ë¯¼ì„œë²—
[RuntimeError]: element 0 of tensors does not require grad and does not have a grad_fn
```

```python
@ìŠ¹ì—°ë²— ì •ì—°ë²—
# stateë¥¼ í†µí•´ ì˜ˆì¸¡í•œ Q-value
        pred = self.model(states)

        # next stateë¥¼ í†µí•´ ê³„ì‚°í•œ ìµœëŒ€ Q-value
        target_pred = self.target_model(next_states).max(1)[0].detach()

        # ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì„ í†µí•œ ì—…ë°ì´íŠ¸ íƒ€ê¹ƒ
        targets = rewards+(1-dones)*self.discount_factor*target_pred
        loss = self.loss(pred.gather(1, actions.unsqueeze(1)), targets.unsqueeze(1))
```
ğŸ‘ : `detach`ë¡œ ë–¼ì–´ì„œ target valueê°€ í•™ìŠµëŒ€ìƒì´ ë˜ì§€ ì•Šë„ë¡ í–ˆë‹¤! ë‹¤ë§Œ ê°€ë…ì„±ì„ ìœ„í•´ loss_fn ì•ˆì—ëŠ” ì°¨ì›ì²˜ë¦¬ ë° ë³€í™˜ì„ ëë‚¸ pred, trg ê°’ ë§Œì„ ë„£ëŠ” ê²ƒì´ ì¢‹ë‹¤. ë˜í•œ íƒ€ê²Ÿì´ ë˜ëŠ” ì‹ ê²½ë§ì€ í•™ìŠµ ëŒ€ìƒì´ ì•„ë‹˜ì„ ëª…í™•íˆ ë³´ì´ê¸° ìœ„í•´ ìœ„ì™€ ê°™ì€ ì½”ë“œë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤!

```python
@ì§€ë¯¼ë²—
pred = self.model(current_states).reshape(-1,self.env.n_actions)
        pred_q_values = pred.gather(1, action_batch) # action idxì˜ ë°ì´í„°ë§Œ êº¼ëƒ„

        # target ê°’ ê³„ì‚° : reward + gamma * Q(s',a')
        with torch.no_grad():
            next_q_values = self.target_model(next_states).max(2).values

        target_q_values = reward_batch + (torch.ones(next_q_values.shape, device=device) - done_batch) * self.discount * next_q_values

        loss = self.loss_fn(pred_q_values, target_q_values)

        running_loss = loss.item()
        self.losses.append(round(running_loss,6))

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

- vector type 

```python
@ ì€ë‚˜ë²— ë„í¬ë²— 
def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return

        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = (reward + self.gamma * torch.max(self.model(torch.FloatTensor(next_state))).item())
            target_f = self.model(torch.FloatTensor(state)).squeeze().tolist()
            if action < self.action_size:  # actionì´ action_size ì´ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                target_f[action] = target
                self.model.zero_grad()
                loss = nn.MSELoss()(self.model(torch.FloatTensor(state)), torch.FloatTensor(target_f))
                loss.backward()
                with torch.no_grad():
                    for param in self.model.parameters():
                        param.data -= 0.001 * param.grad  # í•™ìŠµë¥  0.001ë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

ğŸ’¡ 
1. `if action < self.action_size:  # actionì´ action_size ì´ë‚´ì— ìˆëŠ”ì§€ í™•ì¸`  
ì´ ì½”ë“œëŠ” env-agent ì˜ì¡´ì„±ì„ ê³ ë ¤í•´ ì½”ë“œë¥¼ ì§œë©´ í•„ìš”í•˜ì§€ ì•Šì€ ë¶€ë¶„ì´ë‹¤.  
2. `loss = nn.MSELoss()(self.model(torch.FloatTensor(state)), torch.FloatTensor(target_f))` ì½”ë“œ ìƒì— í‹€ë¦° ë‚´ìš©ì€ ì—†ì§€ë§Œ, 
loss í•¨ìˆ˜ëŠ” `__init__` ë‹¨ì—ì„œ ë”°ë¡œ ì§€ì •í•´ì£¼ì–´ ë¶„ë¦¬ì‹œí‚¤ëŠ”ê²Œ ë” ê°€ë…ì„±ì´ ì¢‹ì€ ì½”ë“œë‹¤!  

ğŸ‘ í•™ìŠµë¥  ê°ì†Œ ë¶€ë¶„ : ìœ ì¼í•˜ê²Œ êµ¬í˜„!!!! ê°•í™”í•™ìŠµì˜ í•µì‹¬ì ì¸ ìš”ì†Œ êµ¬í˜„ì„ í•´ëƒˆë‹¤!! ë‹¤ë§Œ ê°ì†Œìœ¨ ë˜í•œ min, decay í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ë¹¼ì„œ ìƒê°í•˜ëŠ” ê²Œ ì¢‹ë‹¤. 

```python
@ ì§€ë¯¼ë²— 
def train(self, done):
        if len(self.replay_memory) < MEM_SIZE_MIN:
            return

        batch = random.sample(self.replay_memory, BATCH_SIZE)

        # calculate new Q
        states, actions, rewards, next_states, epi_dones = zip(*batch)

        current_states = torch.tensor(states, dtype=torch.float).reshape(-1, 1, self.env.n_actions).to(device)
        next_states = torch.tensor(next_states, dtype=torch.float).reshape(-1, 1, self.env.n_actions).to(device)

        actions = torch.tensor(np.array(actions), dtype=torch.int).reshape(-1,1).to(device)
        rewards = torch.tensor(np.array(rewards), dtype=torch.float).reshape(-1,1).to(device)
        epi_dones = torch.tensor(np.array(epi_dones), dtype=torch.float).reshape(-1,1).to(device)

        self.model.train()
        self.target_model.eval()

        current_q_values = self.model(current_states).reshape(-1,self.env.n_actions)

        with torch.no_grad():
            next_q_values = self.target_model(next_states)

        target_value = rewards + (1 - epi_dones) * self.discount * torch.max(next_q_values, dim=2)[0].reshape(-1,1)

        target_q_values = copy.deepcopy(current_q_values.detach())
        target_q_values[range(BATCH_SIZE), actions] = target_value

        loss = self.loss_fn(current_q_values, target_q_values)
        running_loss = loss.item()
        self.losses.append(round(running_loss,6))

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if done:
            self.target_update_counter += 1

        if self.target_update_counter == UPDATE_TARGET_EVERY:
            self.target_update_counter = 0
            self.update_target_model()

        # decay epsilon
        self.epsilon = max(EPSILON_MIN, self.epsilon*EPSILON_DECAY)
```


### Update Target model
```python
@ ì§€ì€ë²— ìŠ¹ì—°ë²—
if e%100 ==0:
    agent.update_target_model()
```

```python
@ì§€ë¯¼ë²—
if done:
    self.target_update_counter += 1

if self.target_update_counter == UPDATE_TARGET_EVERY:
    self.target_update_counter = 0
    self.update_target_model()
```

íƒ€ê²Ÿ ì‹ ê²½ë§ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë¶€ë¶„ì€ ì—í”¼ì†Œë“œ ì‹¤í–‰ ë¶€ë¶„ì— ìˆì–´ë„ ë˜ê³ , agent class ì•ˆì— ìˆì–´ë„ ëœë‹¤. 
ìœ„ ì§€ì€ë²— ìŠ¹ì—°ë²—ì˜ ì½”ë“œê°€ ì²« ë²ˆì§¸ ì¼€ì´ìŠ¤, ì§€ë¯¼ë²—ì˜ ì½”ë“œê°€ ë‘ ë²ˆì§¸ ì¼€ì´ìŠ¤ë‹¤. 
ë‹¤ë§Œ íƒ€ê²Ÿ ì‹ ê²½ë§ì„ í•™ìŠµ ì‹ ê²½ë§ì˜ íŒŒë¼ë¯¸í„°ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒ ë˜í•œ í•˜ë‚˜ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì´ê¸° ë•Œë¬¸ì— í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ëª…ì‹œí•´ë‘ëŠ” ê²ƒì´ ì¢‹ë‹¤. 

### Print 

```python
@ìŠ¹ì—°ë²—
print(f"[episode: {epi+1}/{EPISODES}] score avg: {score_avg:.3f}, cnt: {cnt}, memory length: {len(agent.memory)}, epsilon: {agent.epsilon:.3f}")
>> [episode: 9851/10000] score avg: 0.840, cnt: 8821, memory length: 2000, epsilon: 0.010
```

```python
@ë¯¼ì„œë²—
episodes.append(epi)
scores.append(score)
scores_avg.append(np.mean(scores))

if (epi % 50 == 0):
    print(f"episode: {epi} | lever right:{cnt} | score: {np.mean(scores):.3f}")

>> Episode: episode: 10000 | lever right:9314 | score: 0.896
```
ğŸ’¡ ì´ˆë°˜ ì—í”¼ì†Œë“œëŠ” í•™ìŠµì´ ë˜ì§€ ì•Šì€ ìë£Œì´ê¸° ë•Œë¬¸ì—, ì´ˆë°˜ ì—í”¼ì†Œë“œì˜ ê²°ê³¼ê¹Œì§€ ì „ë¶€ ë‹¤ í¬í•¨í•´ í˜„ì¬ í•™ìŠµì„ ë³´ëŠ”ê±°ëŠ” íš¨ê³¼ì ì´ì§€ ì•Šë‹¤. 
`print_interval`ì„ ì •í•´ íŠ¹ì • êµ¬ê°„ì—ì„œì˜ ê²°ê³¼ì˜ í‰ê· ì„ ë³´ëŠ”ê²Œ í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ì„ ì•Œì•„ë³´ëŠ”ë° ì í•©í•˜ë‹¤.  

## Visualization
- @ìŠ¹ì—°ë²— : 50 interval ê°„ì˜ í‰ê·  scores
![Alt text](image.png)  

- @ì§€ë¯¼ë²—  
1. Mean rewards per 100 epi  
![Alt text](image-2.png)
2. Mean Rewards per 100 epi  
![Alt text](image-3.png)
3. Loss  
![Alt text](image-4.png)

## Tester

```python
@ì§€ì€ë²—
# í•™ìŠµëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸
print("---Test---")
state = env.reset()
print("initial state : ", state)
done = False
while not done:
    action = agent.get_action(state,False)
    next_state, reward, done = env.step(action)
    print("Action:", action)
    print("Next State:", next_state)
    print("Reward:", reward)
    state = next_state

>> ---Test---
>> initial state :  [0, 0, 0]
>> Action: 2
>> Next State: [0, 0, 1]
>> Reward: 1
```
- ë‹¨ì¼ ì—í”¼ì†Œë“œ ê²€ì • ì½”ë“œ

```python
@ì£¼í˜„ë²—
# Test
def test_model(model_path, env, agent, num_episodes=100):
    # ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œ
    agent.model.load_state_dict(torch.load(model_path))
    agent.model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ
    agent.epsilon = 0 # í‰ê°€ëª¨ë“œì—ì„œ epsilon = 0 ì„¤ì •

    # ì—í”¼ì†Œë“œë³„ ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”
    episode_rewards = []

    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë£¨í”„ë¥¼ ì‹¤í–‰
    for episode in range(1, num_episodes + 1):
        state = env.reset()
        done = False
        total_reward = 0

        while not done:
            action = agent.get_action(state)
            next_state, reward, done = env.step(action)

            state = next_state
            total_reward += reward

        # ì—í”¼ì†Œë“œë³„ ì ìˆ˜ ê¸°ë¡
        episode_rewards.append(total_reward)

    # ê²°ê³¼ë¥¼ ì¶œë ¥
    avg_reward = sum(episode_rewards) / num_episodes
    print(f"Avg. Reward over {num_episodes} rewards: {avg_reward}")

    # í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    plt.plot(episode_rewards)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title('Reward per Episode')
    plt.show()

# ëª¨ë¸ ê²½ë¡œì™€ í…ŒìŠ¤íŠ¸í•  í™˜ê²½ì„ ì„¤ì •
model_path = 'trained_model.pth'
env = Environment()
agent = MonkeyAgent(len(env.observation_space), len(env.action_space))

# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸
test_model(model_path, env, agent)
```
- ì—¬ëŸ¬ ì—í”¼ì†Œë“œì— ëŒ€í•œ Test  

![Alt text](image-1.png)
